{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary\n",
    "from fvcore.nn import FlopCountAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "\n",
    "# Define the bottleneck block for ResNet\n",
    "class BottleneckBlock(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BottleneckBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "class ResNetStem(nn.Module):\n",
    "    def __init__(self, out_channels_=3, out_channels=64):\n",
    "        super(ResNetStem, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(out_channels_, out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)        \n",
    "        out = self.maxpool(out)\n",
    "        return out\n",
    "\n",
    "# Define the ResNet-50 model\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=1000, in_channels=3, num_layers=64, block=BottleneckBlock, stem=ResNetStem, layers=[3, 4, 6, 3]):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.stem = stem(in_channels, num_layers)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, num_layers, layers[0], stride=1)\n",
    "        self.layer2 = self.make_layer(block, num_layers*2, layers[1], stride=2)\n",
    "        self.layer3 = self.make_layer(block, num_layers*4, layers[2], stride=2)\n",
    "        self.layer4 = self.make_layer(block, num_layers*8, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(num_layers * 8 * BottleneckBlock.expansion, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = []\n",
    "        layers.append(block(self.num_layers, out_channels, stride))\n",
    "        self.num_layers = out_channels * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.num_layers, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 69 time(s)\n",
      "Unsupported operator aten::max_pool2d encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.14e+09\n",
      "conv 4.09e+09\n",
      "batch_norm 5.56e+07\n",
      "adaptive_avg_pool2d 1.00e+05\n",
      "linear 2.05e+06\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the ResNet-50 model\n",
    "model = ResNet50(num_classes=1000)\n",
    "\n",
    "# Print the model architecture\n",
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "        ResNetStem-5           [-1, 64, 56, 56]               0\n",
      "            Conv2d-6           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "             ReLU-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-13          [-1, 256, 56, 56]             512\n",
      "           Conv2d-14          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-15          [-1, 256, 56, 56]             512\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-17          [-1, 256, 56, 56]               0\n",
      "           Conv2d-18           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "           Conv2d-21           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "           Conv2d-24          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-25          [-1, 256, 56, 56]             512\n",
      "         Identity-26          [-1, 256, 56, 56]               0\n",
      "             ReLU-27          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-28          [-1, 256, 56, 56]               0\n",
      "           Conv2d-29           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "           Conv2d-32           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
      "             ReLU-34           [-1, 64, 56, 56]               0\n",
      "           Conv2d-35          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-36          [-1, 256, 56, 56]             512\n",
      "         Identity-37          [-1, 256, 56, 56]               0\n",
      "             ReLU-38          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-39          [-1, 256, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-41          [-1, 128, 56, 56]             256\n",
      "             ReLU-42          [-1, 128, 56, 56]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-48          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-50          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-51          [-1, 512, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-56          [-1, 128, 28, 28]             256\n",
      "             ReLU-57          [-1, 128, 28, 28]               0\n",
      "           Conv2d-58          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-59          [-1, 512, 28, 28]           1,024\n",
      "         Identity-60          [-1, 512, 28, 28]               0\n",
      "             ReLU-61          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-62          [-1, 512, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n",
      "         Identity-71          [-1, 512, 28, 28]               0\n",
      "             ReLU-72          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-73          [-1, 512, 28, 28]               0\n",
      "           Conv2d-74          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-75          [-1, 128, 28, 28]             256\n",
      "             ReLU-76          [-1, 128, 28, 28]               0\n",
      "           Conv2d-77          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-78          [-1, 128, 28, 28]             256\n",
      "             ReLU-79          [-1, 128, 28, 28]               0\n",
      "           Conv2d-80          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-81          [-1, 512, 28, 28]           1,024\n",
      "         Identity-82          [-1, 512, 28, 28]               0\n",
      "             ReLU-83          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-84          [-1, 512, 28, 28]               0\n",
      "           Conv2d-85          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-86          [-1, 256, 28, 28]             512\n",
      "             ReLU-87          [-1, 256, 28, 28]               0\n",
      "           Conv2d-88          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-89          [-1, 256, 14, 14]             512\n",
      "             ReLU-90          [-1, 256, 14, 14]               0\n",
      "           Conv2d-91         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-92         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-93         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-94         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-95         [-1, 1024, 14, 14]               0\n",
      "  BottleneckBlock-96         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-97          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-98          [-1, 256, 14, 14]             512\n",
      "             ReLU-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-101          [-1, 256, 14, 14]             512\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "          Conv2d-103         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-104         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-105         [-1, 1024, 14, 14]               0\n",
      "            ReLU-106         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-107         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-108          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-109          [-1, 256, 14, 14]             512\n",
      "            ReLU-110          [-1, 256, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-115         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-116         [-1, 1024, 14, 14]               0\n",
      "            ReLU-117         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-118         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-119          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-120          [-1, 256, 14, 14]             512\n",
      "            ReLU-121          [-1, 256, 14, 14]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-127         [-1, 1024, 14, 14]               0\n",
      "            ReLU-128         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-129         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-130          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-131          [-1, 256, 14, 14]             512\n",
      "            ReLU-132          [-1, 256, 14, 14]               0\n",
      "          Conv2d-133          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-134          [-1, 256, 14, 14]             512\n",
      "            ReLU-135          [-1, 256, 14, 14]               0\n",
      "          Conv2d-136         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-137         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-138         [-1, 1024, 14, 14]               0\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-149         [-1, 1024, 14, 14]               0\n",
      "            ReLU-150         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-151         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-152          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-153          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-154          [-1, 512, 14, 14]               0\n",
      "          Conv2d-155            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-156            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-157            [-1, 512, 7, 7]               0\n",
      "          Conv2d-158           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-159           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-160           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-161           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-162           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-163           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-164            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-165            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-166            [-1, 512, 7, 7]               0\n",
      "          Conv2d-167            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-168            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-169            [-1, 512, 7, 7]               0\n",
      "          Conv2d-170           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-171           [-1, 2048, 7, 7]           4,096\n",
      "        Identity-172           [-1, 2048, 7, 7]               0\n",
      "            ReLU-173           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-174           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-175            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-176            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-177            [-1, 512, 7, 7]               0\n",
      "          Conv2d-178            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-179            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-180            [-1, 512, 7, 7]               0\n",
      "          Conv2d-181           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-182           [-1, 2048, 7, 7]           4,096\n",
      "        Identity-183           [-1, 2048, 7, 7]               0\n",
      "            ReLU-184           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-185           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-186           [-1, 2048, 1, 1]               0\n",
      "          Linear-187                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 25,557,032\n",
      "Trainable params: 25,557,032\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 318.71\n",
      "Params size (MB): 97.49\n",
      "Estimated Total Size (MB): 416.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224), device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1. Changing Compute Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 77 time(s)\n",
      "Unsupported operator aten::max_pool2d encountered 1 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.58e+09\n",
      "conv 4.52e+09\n",
      "batch_norm 5.71e+07\n",
      "adaptive_avg_pool2d 1.00e+05\n",
      "linear 2.05e+06\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=1000, in_channels=3, num_layers=64, block=BottleneckBlock, stem=ResNetStem, layers=[3, 3, 9, 3])\n",
    "# Print the model architecture\n",
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "        ResNetStem-5           [-1, 64, 56, 56]               0\n",
      "            Conv2d-6           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-7           [-1, 64, 56, 56]             128\n",
      "              ReLU-8           [-1, 64, 56, 56]               0\n",
      "            Conv2d-9           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-10           [-1, 64, 56, 56]             128\n",
      "             ReLU-11           [-1, 64, 56, 56]               0\n",
      "           Conv2d-12          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-13          [-1, 256, 56, 56]             512\n",
      "           Conv2d-14          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-15          [-1, 256, 56, 56]             512\n",
      "             ReLU-16          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-17          [-1, 256, 56, 56]               0\n",
      "           Conv2d-18           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-19           [-1, 64, 56, 56]             128\n",
      "             ReLU-20           [-1, 64, 56, 56]               0\n",
      "           Conv2d-21           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-22           [-1, 64, 56, 56]             128\n",
      "             ReLU-23           [-1, 64, 56, 56]               0\n",
      "           Conv2d-24          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-25          [-1, 256, 56, 56]             512\n",
      "         Identity-26          [-1, 256, 56, 56]               0\n",
      "             ReLU-27          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-28          [-1, 256, 56, 56]               0\n",
      "           Conv2d-29           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-30           [-1, 64, 56, 56]             128\n",
      "             ReLU-31           [-1, 64, 56, 56]               0\n",
      "           Conv2d-32           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-33           [-1, 64, 56, 56]             128\n",
      "             ReLU-34           [-1, 64, 56, 56]               0\n",
      "           Conv2d-35          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-36          [-1, 256, 56, 56]             512\n",
      "         Identity-37          [-1, 256, 56, 56]               0\n",
      "             ReLU-38          [-1, 256, 56, 56]               0\n",
      "  BottleneckBlock-39          [-1, 256, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-41          [-1, 128, 56, 56]             256\n",
      "             ReLU-42          [-1, 128, 56, 56]               0\n",
      "           Conv2d-43          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-44          [-1, 128, 28, 28]             256\n",
      "             ReLU-45          [-1, 128, 28, 28]               0\n",
      "           Conv2d-46          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-47          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-48          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-49          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-50          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-51          [-1, 512, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-56          [-1, 128, 28, 28]             256\n",
      "             ReLU-57          [-1, 128, 28, 28]               0\n",
      "           Conv2d-58          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-59          [-1, 512, 28, 28]           1,024\n",
      "         Identity-60          [-1, 512, 28, 28]               0\n",
      "             ReLU-61          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-62          [-1, 512, 28, 28]               0\n",
      "           Conv2d-63          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-64          [-1, 128, 28, 28]             256\n",
      "             ReLU-65          [-1, 128, 28, 28]               0\n",
      "           Conv2d-66          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-67          [-1, 128, 28, 28]             256\n",
      "             ReLU-68          [-1, 128, 28, 28]               0\n",
      "           Conv2d-69          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 512, 28, 28]           1,024\n",
      "         Identity-71          [-1, 512, 28, 28]               0\n",
      "             ReLU-72          [-1, 512, 28, 28]               0\n",
      "  BottleneckBlock-73          [-1, 512, 28, 28]               0\n",
      "           Conv2d-74          [-1, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-75          [-1, 256, 28, 28]             512\n",
      "             ReLU-76          [-1, 256, 28, 28]               0\n",
      "           Conv2d-77          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-78          [-1, 256, 14, 14]             512\n",
      "             ReLU-79          [-1, 256, 14, 14]               0\n",
      "           Conv2d-80         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-81         [-1, 1024, 14, 14]           2,048\n",
      "           Conv2d-82         [-1, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-83         [-1, 1024, 14, 14]           2,048\n",
      "             ReLU-84         [-1, 1024, 14, 14]               0\n",
      "  BottleneckBlock-85         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-86          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-87          [-1, 256, 14, 14]             512\n",
      "             ReLU-88          [-1, 256, 14, 14]               0\n",
      "           Conv2d-89          [-1, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-90          [-1, 256, 14, 14]             512\n",
      "             ReLU-91          [-1, 256, 14, 14]               0\n",
      "           Conv2d-92         [-1, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-93         [-1, 1024, 14, 14]           2,048\n",
      "         Identity-94         [-1, 1024, 14, 14]               0\n",
      "             ReLU-95         [-1, 1024, 14, 14]               0\n",
      "  BottleneckBlock-96         [-1, 1024, 14, 14]               0\n",
      "           Conv2d-97          [-1, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-98          [-1, 256, 14, 14]             512\n",
      "             ReLU-99          [-1, 256, 14, 14]               0\n",
      "          Conv2d-100          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-101          [-1, 256, 14, 14]             512\n",
      "            ReLU-102          [-1, 256, 14, 14]               0\n",
      "          Conv2d-103         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-104         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-105         [-1, 1024, 14, 14]               0\n",
      "            ReLU-106         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-107         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-108          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-109          [-1, 256, 14, 14]             512\n",
      "            ReLU-110          [-1, 256, 14, 14]               0\n",
      "          Conv2d-111          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-112          [-1, 256, 14, 14]             512\n",
      "            ReLU-113          [-1, 256, 14, 14]               0\n",
      "          Conv2d-114         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-115         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-116         [-1, 1024, 14, 14]               0\n",
      "            ReLU-117         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-118         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-119          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-120          [-1, 256, 14, 14]             512\n",
      "            ReLU-121          [-1, 256, 14, 14]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-127         [-1, 1024, 14, 14]               0\n",
      "            ReLU-128         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-129         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-130          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-131          [-1, 256, 14, 14]             512\n",
      "            ReLU-132          [-1, 256, 14, 14]               0\n",
      "          Conv2d-133          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-134          [-1, 256, 14, 14]             512\n",
      "            ReLU-135          [-1, 256, 14, 14]               0\n",
      "          Conv2d-136         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-137         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-138         [-1, 1024, 14, 14]               0\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-149         [-1, 1024, 14, 14]               0\n",
      "            ReLU-150         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-151         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-152          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-153          [-1, 256, 14, 14]             512\n",
      "            ReLU-154          [-1, 256, 14, 14]               0\n",
      "          Conv2d-155          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-156          [-1, 256, 14, 14]             512\n",
      "            ReLU-157          [-1, 256, 14, 14]               0\n",
      "          Conv2d-158         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-159         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-160         [-1, 1024, 14, 14]               0\n",
      "            ReLU-161         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-162         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-163          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-164          [-1, 256, 14, 14]             512\n",
      "            ReLU-165          [-1, 256, 14, 14]               0\n",
      "          Conv2d-166          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-167          [-1, 256, 14, 14]             512\n",
      "            ReLU-168          [-1, 256, 14, 14]               0\n",
      "          Conv2d-169         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-170         [-1, 1024, 14, 14]           2,048\n",
      "        Identity-171         [-1, 1024, 14, 14]               0\n",
      "            ReLU-172         [-1, 1024, 14, 14]               0\n",
      " BottleneckBlock-173         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-174          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-175          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-176          [-1, 512, 14, 14]               0\n",
      "          Conv2d-177            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-178            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-179            [-1, 512, 7, 7]               0\n",
      "          Conv2d-180           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-181           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-182           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-183           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-184           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-185           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-186            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-187            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-188            [-1, 512, 7, 7]               0\n",
      "          Conv2d-189            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-190            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-191            [-1, 512, 7, 7]               0\n",
      "          Conv2d-192           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-193           [-1, 2048, 7, 7]           4,096\n",
      "        Identity-194           [-1, 2048, 7, 7]               0\n",
      "            ReLU-195           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-196           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-197            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-198            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-199            [-1, 512, 7, 7]               0\n",
      "          Conv2d-200            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-201            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-202            [-1, 512, 7, 7]               0\n",
      "          Conv2d-203           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-204           [-1, 2048, 7, 7]           4,096\n",
      "        Identity-205           [-1, 2048, 7, 7]               0\n",
      "            ReLU-206           [-1, 2048, 7, 7]               0\n",
      " BottleneckBlock-207           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-208           [-1, 2048, 1, 1]               0\n",
      "          Linear-209                 [-1, 1000]       2,049,000\n",
      "================================================================\n",
      "Total params: 28,628,520\n",
      "Trainable params: 28,628,520\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 328.67\n",
      "Params size (MB): 109.21\n",
      "Estimated Total Size (MB): 438.45\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patchify stem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further we will use LayerNorm instead of BatchNorm, above is the implementation from ConvNext repo, it supports channel_first mode (N, C, H, W), but official PyTorch implementation only operates on channel_last (N, H, W, C), hence we permute for each layernorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 224, 224, 16])"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.LayerNorm((16,))(torch.randn(20, 16, 224, 224).permute(0, 2, 3, 1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "class patchify_stem(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=64):\n",
    "        super(patchify_stem, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=4, padding=0, bias=False)\n",
    "        self.bn1 = nn.LayerNorm(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x).permute(0, 2, 3, 1)\n",
    "        out = self.bn1(out).permute(0, 3, 1, 2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 76 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47e+09\n",
      "conv 4.42e+09\n",
      "layer_norm 1.00e+06\n",
      "batch_norm 5.31e+07\n",
      "adaptive_avg_pool2d 1.00e+05\n",
      "linear 2.05e+06\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=1000, in_channels=3, num_layers=64, block=BottleneckBlock, stem=patchify_stem, layers=[3, 3, 9, 3])\n",
    "# Print the model architecture\n",
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNeXT-ify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure ResNeXT approach - BN + ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the bottleneck block for ResNet\n",
    "class ResNeXTBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNeXTBottleneck, self).__init__()\n",
    "        # Pointwise convolution - channel reduction and mixing\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Depthwise convolution - spatial mixing\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False, groups=out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        # Pointwise convolution - channel restoration and mixing\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer-like approach - minimal activations (GELU) + LayerNorm instead of BN + ReLU after each conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the bottleneck block for ResNet\n",
    "class ResNeXTBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResNeXTBottleneck, self).__init__()\n",
    "        # Pointwise convolution - channel reduction and mixing\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        # Depthwise convolution - spatial mixing\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False, groups=out_channels)\n",
    "        self.bn2 = nn.LayerNorm(out_channels)\n",
    "        # Pointwise convolution - channel restoration and mixing\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels * self.expansion, kernel_size=1, bias=False)\n",
    "        self.relu = nn.GELU()\n",
    "\n",
    "        self.shortcut = nn.Identity()\n",
    "        if stride != 1 or in_channels != out_channels * self.expansion:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels * self.expansion, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels * self.expansion)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out).permute(0, 2, 3, 1)\n",
    "        out = self.bn2(out).permute(0, 3, 1, 2)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 76 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.40e+09\n",
      "conv 2.35e+09\n",
      "layer_norm 1.00e+06\n",
      "batch_norm 5.31e+07\n",
      "adaptive_avg_pool2d 1.00e+05\n",
      "linear 2.05e+06\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=1000, in_channels=3, num_layers=64, block=ResNeXTBottleneck, stem=patchify_stem, layers=[3, 3, 9, 3])\n",
    "# Print the model architecture\n",
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase the num_channels to 96 to bring back the FLOPS and improve accuract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::add_ encountered 76 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.35e+09\n",
      "conv 5.26e+09\n",
      "layer_norm 1.51e+06\n",
      "batch_norm 7.96e+07\n",
      "adaptive_avg_pool2d 1.51e+05\n",
      "linear 3.07e+06\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=1000, in_channels=3, num_layers=96, block=ResNeXTBottleneck, stem=patchify_stem, layers=[3, 3, 9, 3])\n",
    "# Print the model architecture\n",
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the bottleneck block for ResNet\n",
    "class InvertedBottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3):\n",
    "        super(InvertedBottleneck, self).__init__()\n",
    "        # Pointwise convolution - channel expansion and mixing\n",
    "        self.conv2 = nn.Conv2d(out_channels, in_channels, kernel_size=1, bias=False)\n",
    "        # Depthwise convolution - spatial mixing\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=in_channels, bias=False)\n",
    "        self.bn1 = nn.LayerNorm(in_channels)\n",
    "\n",
    "        # Pointwise convolution - channel restoration and mixing\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.relu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv2(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv1(out).permute(0, 2, 3, 1)\n",
    "        out = self.bn1(out).permute(0, 3, 1, 2)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out += residual\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the ResNet-50 model\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=1000, in_channels=3, num_layers=64, block=BottleneckBlock, stem=ResNetStem, layers=[3, 4, 6, 3], kernel_size=7):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        self.stem = stem(in_channels, num_layers)\n",
    "\n",
    "        self.layer1 = self.make_layer(block, num_layers, layers[0], stride=1)\n",
    "        self.downln1 = nn.LayerNorm(num_layers)\n",
    "        self.downconv1 = nn.Conv2d(num_layers, num_layers*2, kernel_size=2, stride=2, bias=False)\n",
    "        self.layer2 = self.make_layer(block, num_layers*2, layers[1], stride=2)\n",
    "        self.downln2 = nn.LayerNorm(num_layers*2)\n",
    "        self.downconv2 = nn.Conv2d(num_layers*2, num_layers*4, kernel_size=2, stride=2, bias=False)\n",
    "        self.layer3 = self.make_layer(block, num_layers*4, layers[2], stride=2)\n",
    "        self.downln3 = nn.LayerNorm(num_layers*4)\n",
    "        self.downconv3 = nn.Conv2d(num_layers*4, num_layers*8, kernel_size=2, stride=2, bias=False)\n",
    "        self.layer4 = self.make_layer(block, num_layers*8, layers[3], stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(num_layers * 2 * BottleneckBlock.expansion, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_channels, blocks, stride=1):\n",
    "        layers = nn.Sequential()\n",
    "        for _ in range(0, blocks):\n",
    "            layers.append(block(out_channels*4, out_channels, kernel_size=self.kernel_size))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.stem(x)\n",
    "        out = self.layer1(out).permute(0, 2, 3, 1)\n",
    "        out = self.downln1(out).permute(0, 3, 1, 2)\n",
    "        out = self.downconv1(out)\n",
    "        \n",
    "        out = self.layer2(out).permute(0, 2, 3, 1)\n",
    "        out = self.downln2(out).permute(0, 3, 1, 2)\n",
    "        out = self.downconv2(out)\n",
    "        \n",
    "        out = self.layer3(out).permute(0, 2, 3, 1)\n",
    "        out = self.downln3(out).permute(0, 3, 1, 2)\n",
    "        out = self.downconv3(out)\n",
    "        \n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,608\n",
      "         LayerNorm-2           [-1, 56, 56, 96]             192\n",
      "     patchify_stem-3           [-1, 96, 56, 56]               0\n",
      "            Conv2d-4          [-1, 384, 56, 56]          36,864\n",
      "              GELU-5          [-1, 384, 56, 56]               0\n",
      "            Conv2d-6          [-1, 384, 56, 56]           3,456\n",
      "         LayerNorm-7          [-1, 56, 56, 384]             768\n",
      "            Conv2d-8           [-1, 96, 56, 56]          36,864\n",
      "InvertedBottleneck-9           [-1, 96, 56, 56]               0\n",
      "           Conv2d-10          [-1, 384, 56, 56]          36,864\n",
      "             GELU-11          [-1, 384, 56, 56]               0\n",
      "           Conv2d-12          [-1, 384, 56, 56]           3,456\n",
      "        LayerNorm-13          [-1, 56, 56, 384]             768\n",
      "           Conv2d-14           [-1, 96, 56, 56]          36,864\n",
      "InvertedBottleneck-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16          [-1, 384, 56, 56]          36,864\n",
      "             GELU-17          [-1, 384, 56, 56]               0\n",
      "           Conv2d-18          [-1, 384, 56, 56]           3,456\n",
      "        LayerNorm-19          [-1, 56, 56, 384]             768\n",
      "           Conv2d-20           [-1, 96, 56, 56]          36,864\n",
      "InvertedBottleneck-21           [-1, 96, 56, 56]               0\n",
      "        LayerNorm-22           [-1, 56, 56, 96]             192\n",
      "           Conv2d-23          [-1, 192, 28, 28]          73,728\n",
      "           Conv2d-24          [-1, 768, 28, 28]         147,456\n",
      "             GELU-25          [-1, 768, 28, 28]               0\n",
      "           Conv2d-26          [-1, 768, 28, 28]           6,912\n",
      "        LayerNorm-27          [-1, 28, 28, 768]           1,536\n",
      "           Conv2d-28          [-1, 192, 28, 28]         147,456\n",
      "InvertedBottleneck-29          [-1, 192, 28, 28]               0\n",
      "           Conv2d-30          [-1, 768, 28, 28]         147,456\n",
      "             GELU-31          [-1, 768, 28, 28]               0\n",
      "           Conv2d-32          [-1, 768, 28, 28]           6,912\n",
      "        LayerNorm-33          [-1, 28, 28, 768]           1,536\n",
      "           Conv2d-34          [-1, 192, 28, 28]         147,456\n",
      "InvertedBottleneck-35          [-1, 192, 28, 28]               0\n",
      "           Conv2d-36          [-1, 768, 28, 28]         147,456\n",
      "             GELU-37          [-1, 768, 28, 28]               0\n",
      "           Conv2d-38          [-1, 768, 28, 28]           6,912\n",
      "        LayerNorm-39          [-1, 28, 28, 768]           1,536\n",
      "           Conv2d-40          [-1, 192, 28, 28]         147,456\n",
      "InvertedBottleneck-41          [-1, 192, 28, 28]               0\n",
      "        LayerNorm-42          [-1, 28, 28, 192]             384\n",
      "           Conv2d-43          [-1, 384, 14, 14]         294,912\n",
      "           Conv2d-44         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-45         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-46         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-47         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-48          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-49          [-1, 384, 14, 14]               0\n",
      "           Conv2d-50         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-51         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-52         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-53         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-54          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-55          [-1, 384, 14, 14]               0\n",
      "           Conv2d-56         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-57         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-58         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-59         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-60          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-61          [-1, 384, 14, 14]               0\n",
      "           Conv2d-62         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-63         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-64         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-65         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-66          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-67          [-1, 384, 14, 14]               0\n",
      "           Conv2d-68         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-69         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-70         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-71         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-72          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-73          [-1, 384, 14, 14]               0\n",
      "           Conv2d-74         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-75         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-76         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-77         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-78          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-79          [-1, 384, 14, 14]               0\n",
      "           Conv2d-80         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-81         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-82         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-83         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-84          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-85          [-1, 384, 14, 14]               0\n",
      "           Conv2d-86         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-87         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-88         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-89         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-90          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-91          [-1, 384, 14, 14]               0\n",
      "           Conv2d-92         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-93         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-94         [-1, 1536, 14, 14]          13,824\n",
      "        LayerNorm-95         [-1, 14, 14, 1536]           3,072\n",
      "           Conv2d-96          [-1, 384, 14, 14]         589,824\n",
      "InvertedBottleneck-97          [-1, 384, 14, 14]               0\n",
      "        LayerNorm-98          [-1, 14, 14, 384]             768\n",
      "           Conv2d-99            [-1, 768, 7, 7]       1,179,648\n",
      "          Conv2d-100           [-1, 3072, 7, 7]       2,359,296\n",
      "            GELU-101           [-1, 3072, 7, 7]               0\n",
      "          Conv2d-102           [-1, 3072, 7, 7]          27,648\n",
      "       LayerNorm-103           [-1, 7, 7, 3072]           6,144\n",
      "          Conv2d-104            [-1, 768, 7, 7]       2,359,296\n",
      "InvertedBottleneck-105            [-1, 768, 7, 7]               0\n",
      "          Conv2d-106           [-1, 3072, 7, 7]       2,359,296\n",
      "            GELU-107           [-1, 3072, 7, 7]               0\n",
      "          Conv2d-108           [-1, 3072, 7, 7]          27,648\n",
      "       LayerNorm-109           [-1, 7, 7, 3072]           6,144\n",
      "          Conv2d-110            [-1, 768, 7, 7]       2,359,296\n",
      "InvertedBottleneck-111            [-1, 768, 7, 7]               0\n",
      "          Conv2d-112           [-1, 3072, 7, 7]       2,359,296\n",
      "            GELU-113           [-1, 3072, 7, 7]               0\n",
      "          Conv2d-114           [-1, 3072, 7, 7]          27,648\n",
      "       LayerNorm-115           [-1, 7, 7, 3072]           6,144\n",
      "          Conv2d-116            [-1, 768, 7, 7]       2,359,296\n",
      "InvertedBottleneck-117            [-1, 768, 7, 7]               0\n",
      "AdaptiveAvgPool2d-118            [-1, 768, 1, 1]               0\n",
      "          Linear-119                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 28,493,416\n",
      "Trainable params: 28,493,416\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 307.51\n",
      "Params size (MB): 108.69\n",
      "Estimated Total Size (MB): 416.78\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(num_classes=1000, in_channels=3, num_layers=96, block=InvertedBottleneck, stem=patchify_stem, layers=[3, 3, 9, 3], kernel_size=3)\n",
    "\n",
    "summary(model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 18 time(s)\n",
      "Unsupported operator aten::add_ encountered 18 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47e+09\n",
      "conv 4.43e+09\n",
      "layer_norm 4.70e+07\n",
      "adaptive_avg_pool2d 3.76e+04\n",
      "linear 7.68e+05\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move spatial mixing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the bottleneck block for ResNet\n",
    "class SpatialFirstBottleNeck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1, kernel_size=3):\n",
    "        super(SpatialFirstBottleNeck, self).__init__()\n",
    "        # Depthwise convolution - spatial mixing\n",
    "        self.conv1 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=out_channels, bias=False)\n",
    "        self.bn1 = nn.LayerNorm(out_channels)\n",
    "        # Pointwise convolution - channel expansion and mixing\n",
    "        self.conv2 = nn.Conv2d(out_channels, in_channels, kernel_size=1, bias=False)\n",
    "        # Pointwise convolution - channel restoration and mixing\n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.relu = nn.GELU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x).permute(0, 2, 3, 1)\n",
    "        out = self.bn1(out).permute(0, 3, 1, 2)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        \n",
    "        out += residual\n",
    "\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "#         super().__init__()\n",
    "#         self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "#         self.norm = LayerNorm(dim, eps=1e-6)\n",
    "#         self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "#         self.act = nn.GELU()\n",
    "#         self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "#         self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "#                                     requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(num_classes=1000, in_channels=3, num_layers=96, block=SpatialFirstBottleNeck, stem=patchify_stem, layers=[3, 3, 9, 3], kernel_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,608\n",
      "         LayerNorm-2           [-1, 56, 56, 96]             192\n",
      "     patchify_stem-3           [-1, 96, 56, 56]               0\n",
      "            Conv2d-4           [-1, 96, 56, 56]             864\n",
      "         LayerNorm-5           [-1, 56, 56, 96]             192\n",
      "            Conv2d-6          [-1, 384, 56, 56]          36,864\n",
      "              GELU-7          [-1, 384, 56, 56]               0\n",
      "            Conv2d-8           [-1, 96, 56, 56]          36,864\n",
      "SpatialFirstBottleNeck-9           [-1, 96, 56, 56]               0\n",
      "           Conv2d-10           [-1, 96, 56, 56]             864\n",
      "        LayerNorm-11           [-1, 56, 56, 96]             192\n",
      "           Conv2d-12          [-1, 384, 56, 56]          36,864\n",
      "             GELU-13          [-1, 384, 56, 56]               0\n",
      "           Conv2d-14           [-1, 96, 56, 56]          36,864\n",
      "SpatialFirstBottleNeck-15           [-1, 96, 56, 56]               0\n",
      "           Conv2d-16           [-1, 96, 56, 56]             864\n",
      "        LayerNorm-17           [-1, 56, 56, 96]             192\n",
      "           Conv2d-18          [-1, 384, 56, 56]          36,864\n",
      "             GELU-19          [-1, 384, 56, 56]               0\n",
      "           Conv2d-20           [-1, 96, 56, 56]          36,864\n",
      "SpatialFirstBottleNeck-21           [-1, 96, 56, 56]               0\n",
      "        LayerNorm-22           [-1, 56, 56, 96]             192\n",
      "           Conv2d-23          [-1, 192, 28, 28]          73,728\n",
      "           Conv2d-24          [-1, 192, 28, 28]           1,728\n",
      "        LayerNorm-25          [-1, 28, 28, 192]             384\n",
      "           Conv2d-26          [-1, 768, 28, 28]         147,456\n",
      "             GELU-27          [-1, 768, 28, 28]               0\n",
      "           Conv2d-28          [-1, 192, 28, 28]         147,456\n",
      "SpatialFirstBottleNeck-29          [-1, 192, 28, 28]               0\n",
      "           Conv2d-30          [-1, 192, 28, 28]           1,728\n",
      "        LayerNorm-31          [-1, 28, 28, 192]             384\n",
      "           Conv2d-32          [-1, 768, 28, 28]         147,456\n",
      "             GELU-33          [-1, 768, 28, 28]               0\n",
      "           Conv2d-34          [-1, 192, 28, 28]         147,456\n",
      "SpatialFirstBottleNeck-35          [-1, 192, 28, 28]               0\n",
      "           Conv2d-36          [-1, 192, 28, 28]           1,728\n",
      "        LayerNorm-37          [-1, 28, 28, 192]             384\n",
      "           Conv2d-38          [-1, 768, 28, 28]         147,456\n",
      "             GELU-39          [-1, 768, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]         147,456\n",
      "SpatialFirstBottleNeck-41          [-1, 192, 28, 28]               0\n",
      "        LayerNorm-42          [-1, 28, 28, 192]             384\n",
      "           Conv2d-43          [-1, 384, 14, 14]         294,912\n",
      "           Conv2d-44          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-45          [-1, 14, 14, 384]             768\n",
      "           Conv2d-46         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-47         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-48          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-49          [-1, 384, 14, 14]               0\n",
      "           Conv2d-50          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-51          [-1, 14, 14, 384]             768\n",
      "           Conv2d-52         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-53         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-54          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-55          [-1, 384, 14, 14]               0\n",
      "           Conv2d-56          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-57          [-1, 14, 14, 384]             768\n",
      "           Conv2d-58         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-59         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-60          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-61          [-1, 384, 14, 14]               0\n",
      "           Conv2d-62          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-63          [-1, 14, 14, 384]             768\n",
      "           Conv2d-64         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-65         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-66          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-67          [-1, 384, 14, 14]               0\n",
      "           Conv2d-68          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-69          [-1, 14, 14, 384]             768\n",
      "           Conv2d-70         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-71         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-72          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-73          [-1, 384, 14, 14]               0\n",
      "           Conv2d-74          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-75          [-1, 14, 14, 384]             768\n",
      "           Conv2d-76         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-77         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-78          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-79          [-1, 384, 14, 14]               0\n",
      "           Conv2d-80          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-81          [-1, 14, 14, 384]             768\n",
      "           Conv2d-82         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-83         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-84          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-85          [-1, 384, 14, 14]               0\n",
      "           Conv2d-86          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-87          [-1, 14, 14, 384]             768\n",
      "           Conv2d-88         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-89         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-90          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-91          [-1, 384, 14, 14]               0\n",
      "           Conv2d-92          [-1, 384, 14, 14]           3,456\n",
      "        LayerNorm-93          [-1, 14, 14, 384]             768\n",
      "           Conv2d-94         [-1, 1536, 14, 14]         589,824\n",
      "             GELU-95         [-1, 1536, 14, 14]               0\n",
      "           Conv2d-96          [-1, 384, 14, 14]         589,824\n",
      "SpatialFirstBottleNeck-97          [-1, 384, 14, 14]               0\n",
      "        LayerNorm-98          [-1, 14, 14, 384]             768\n",
      "           Conv2d-99            [-1, 768, 7, 7]       1,179,648\n",
      "          Conv2d-100            [-1, 768, 7, 7]           6,912\n",
      "       LayerNorm-101            [-1, 7, 7, 768]           1,536\n",
      "          Conv2d-102           [-1, 3072, 7, 7]       2,359,296\n",
      "            GELU-103           [-1, 3072, 7, 7]               0\n",
      "          Conv2d-104            [-1, 768, 7, 7]       2,359,296\n",
      "SpatialFirstBottleNeck-105            [-1, 768, 7, 7]               0\n",
      "          Conv2d-106            [-1, 768, 7, 7]           6,912\n",
      "       LayerNorm-107            [-1, 7, 7, 768]           1,536\n",
      "          Conv2d-108           [-1, 3072, 7, 7]       2,359,296\n",
      "            GELU-109           [-1, 3072, 7, 7]               0\n",
      "          Conv2d-110            [-1, 768, 7, 7]       2,359,296\n",
      "SpatialFirstBottleNeck-111            [-1, 768, 7, 7]               0\n",
      "          Conv2d-112            [-1, 768, 7, 7]           6,912\n",
      "       LayerNorm-113            [-1, 7, 7, 768]           1,536\n",
      "          Conv2d-114           [-1, 3072, 7, 7]       2,359,296\n",
      "            GELU-115           [-1, 3072, 7, 7]               0\n",
      "          Conv2d-116            [-1, 768, 7, 7]       2,359,296\n",
      "SpatialFirstBottleNeck-117            [-1, 768, 7, 7]               0\n",
      "AdaptiveAvgPool2d-118            [-1, 768, 1, 1]               0\n",
      "          Linear-119                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 28,274,824\n",
      "Trainable params: 28,274,824\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 209.32\n",
      "Params size (MB): 107.86\n",
      "Estimated Total Size (MB): 317.75\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::gelu encountered 18 time(s)\n",
      "Unsupported operator aten::add_ encountered 18 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.38e+09\n",
      "conv 4.37e+09\n",
      "layer_norm 1.49e+07\n",
      "adaptive_avg_pool2d 3.76e+04\n",
      "linear 7.68e+05\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus Zephyrus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\Asus Zephyrus\\AppData\\Local\\Temp\\ipykernel_24844\\4269841588.py:158: UserWarning: Overwriting convnext_tiny in registry with __main__.convnext_tiny. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\Asus Zephyrus\\AppData\\Local\\Temp\\ipykernel_24844\\4269841588.py:167: UserWarning: Overwriting convnext_small in registry with __main__.convnext_small. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\Asus Zephyrus\\AppData\\Local\\Temp\\ipykernel_24844\\4269841588.py:176: UserWarning: Overwriting convnext_base in registry with __main__.convnext_base. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\Asus Zephyrus\\AppData\\Local\\Temp\\ipykernel_24844\\4269841588.py:185: UserWarning: Overwriting convnext_large in registry with __main__.convnext_large. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n",
      "C:\\Users\\Asus Zephyrus\\AppData\\Local\\Temp\\ipykernel_24844\\4269841588.py:194: UserWarning: Overwriting convnext_xlarge in registry with __main__.convnext_xlarge. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
      "  @register_model\n"
     ]
    }
   ],
   "source": [
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "\n",
    "# All rights reserved.\n",
    "\n",
    "# This source code is licensed under the license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.registry import register_model\n",
    "\n",
    "class Block(nn.Module):\n",
    "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
    "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
    "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
    "    We use (2) as we find it slightly faster in PyTorch\n",
    "    \n",
    "    Args:\n",
    "        dim (int): Number of input channels.\n",
    "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
    "        self.norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
    "        self.act = nn.GELU()\n",
    "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
    "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
    "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x\n",
    "        x = self.dwconv(x)\n",
    "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
    "        x = self.norm(x)\n",
    "        x = self.pwconv1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pwconv2(x)\n",
    "        if self.gamma is not None:\n",
    "            x = self.gamma * x\n",
    "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
    "\n",
    "        x = input + self.drop_path(x)\n",
    "        return x\n",
    "\n",
    "class ConvNeXt(nn.Module):\n",
    "    r\"\"\" ConvNeXt\n",
    "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
    "          https://arxiv.org/pdf/2201.03545.pdf\n",
    "\n",
    "    Args:\n",
    "        in_chans (int): Number of input image channels. Default: 3\n",
    "        num_classes (int): Number of classes for classification head. Default: 1000\n",
    "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
    "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
    "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
    "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
    "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_chans=3, num_classes=1000, \n",
    "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
    "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
    "        stem = nn.Sequential(\n",
    "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
    "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
    "        )\n",
    "        self.downsample_layers.append(stem)\n",
    "        for i in range(3):\n",
    "            downsample_layer = nn.Sequential(\n",
    "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
    "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
    "            )\n",
    "            self.downsample_layers.append(downsample_layer)\n",
    "\n",
    "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
    "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
    "        cur = 0\n",
    "        for i in range(4):\n",
    "            stage = nn.Sequential(\n",
    "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
    "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
    "            )\n",
    "            self.stages.append(stage)\n",
    "            cur += depths[i]\n",
    "\n",
    "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
    "        self.head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        self.head.weight.data.mul_(head_init_scale)\n",
    "        self.head.bias.data.mul_(head_init_scale)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        for i in range(4):\n",
    "            x = self.downsample_layers[i](x)\n",
    "            x = self.stages[i](x)\n",
    "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.forward_features(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
    "            return x\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
    "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
    "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
    "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
    "    \"convnext_tiny_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_22k_224.pth\",\n",
    "    \"convnext_small_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_22k_224.pth\",\n",
    "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
    "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
    "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
    "}\n",
    "\n",
    "@register_model\n",
    "def convnext_tiny(pretrained=False,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_tiny_22k'] if in_22k else model_urls['convnext_tiny_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_small(pretrained=False,in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_small_22k'] if in_22k else model_urls['convnext_small_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
    "    if pretrained:\n",
    "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model\n",
    "\n",
    "@register_model\n",
    "def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n",
    "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
    "    if pretrained:\n",
    "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
    "        url = model_urls['convnext_xlarge_22k']\n",
    "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
    "        model.load_state_dict(checkpoint[\"model\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 96, 56, 56]           4,704\n",
      "         LayerNorm-2           [-1, 96, 56, 56]             192\n",
      "            Conv2d-3           [-1, 96, 56, 56]           4,800\n",
      "         LayerNorm-4           [-1, 56, 56, 96]             192\n",
      "            Linear-5          [-1, 56, 56, 384]          37,248\n",
      "              GELU-6          [-1, 56, 56, 384]               0\n",
      "            Linear-7           [-1, 56, 56, 96]          36,960\n",
      "          Identity-8           [-1, 96, 56, 56]               0\n",
      "             Block-9           [-1, 96, 56, 56]               0\n",
      "           Conv2d-10           [-1, 96, 56, 56]           4,800\n",
      "        LayerNorm-11           [-1, 56, 56, 96]             192\n",
      "           Linear-12          [-1, 56, 56, 384]          37,248\n",
      "             GELU-13          [-1, 56, 56, 384]               0\n",
      "           Linear-14           [-1, 56, 56, 96]          36,960\n",
      "         Identity-15           [-1, 96, 56, 56]               0\n",
      "            Block-16           [-1, 96, 56, 56]               0\n",
      "           Conv2d-17           [-1, 96, 56, 56]           4,800\n",
      "        LayerNorm-18           [-1, 56, 56, 96]             192\n",
      "           Linear-19          [-1, 56, 56, 384]          37,248\n",
      "             GELU-20          [-1, 56, 56, 384]               0\n",
      "           Linear-21           [-1, 56, 56, 96]          36,960\n",
      "         Identity-22           [-1, 96, 56, 56]               0\n",
      "            Block-23           [-1, 96, 56, 56]               0\n",
      "        LayerNorm-24           [-1, 96, 56, 56]             192\n",
      "           Conv2d-25          [-1, 192, 28, 28]          73,920\n",
      "           Conv2d-26          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-27          [-1, 28, 28, 192]             384\n",
      "           Linear-28          [-1, 28, 28, 768]         148,224\n",
      "             GELU-29          [-1, 28, 28, 768]               0\n",
      "           Linear-30          [-1, 28, 28, 192]         147,648\n",
      "         Identity-31          [-1, 192, 28, 28]               0\n",
      "            Block-32          [-1, 192, 28, 28]               0\n",
      "           Conv2d-33          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-34          [-1, 28, 28, 192]             384\n",
      "           Linear-35          [-1, 28, 28, 768]         148,224\n",
      "             GELU-36          [-1, 28, 28, 768]               0\n",
      "           Linear-37          [-1, 28, 28, 192]         147,648\n",
      "         Identity-38          [-1, 192, 28, 28]               0\n",
      "            Block-39          [-1, 192, 28, 28]               0\n",
      "           Conv2d-40          [-1, 192, 28, 28]           9,600\n",
      "        LayerNorm-41          [-1, 28, 28, 192]             384\n",
      "           Linear-42          [-1, 28, 28, 768]         148,224\n",
      "             GELU-43          [-1, 28, 28, 768]               0\n",
      "           Linear-44          [-1, 28, 28, 192]         147,648\n",
      "         Identity-45          [-1, 192, 28, 28]               0\n",
      "            Block-46          [-1, 192, 28, 28]               0\n",
      "        LayerNorm-47          [-1, 192, 28, 28]             384\n",
      "           Conv2d-48          [-1, 384, 14, 14]         295,296\n",
      "           Conv2d-49          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-50          [-1, 14, 14, 384]             768\n",
      "           Linear-51         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-52         [-1, 14, 14, 1536]               0\n",
      "           Linear-53          [-1, 14, 14, 384]         590,208\n",
      "         Identity-54          [-1, 384, 14, 14]               0\n",
      "            Block-55          [-1, 384, 14, 14]               0\n",
      "           Conv2d-56          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-57          [-1, 14, 14, 384]             768\n",
      "           Linear-58         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-59         [-1, 14, 14, 1536]               0\n",
      "           Linear-60          [-1, 14, 14, 384]         590,208\n",
      "         Identity-61          [-1, 384, 14, 14]               0\n",
      "            Block-62          [-1, 384, 14, 14]               0\n",
      "           Conv2d-63          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-64          [-1, 14, 14, 384]             768\n",
      "           Linear-65         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-66         [-1, 14, 14, 1536]               0\n",
      "           Linear-67          [-1, 14, 14, 384]         590,208\n",
      "         Identity-68          [-1, 384, 14, 14]               0\n",
      "            Block-69          [-1, 384, 14, 14]               0\n",
      "           Conv2d-70          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-71          [-1, 14, 14, 384]             768\n",
      "           Linear-72         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-73         [-1, 14, 14, 1536]               0\n",
      "           Linear-74          [-1, 14, 14, 384]         590,208\n",
      "         Identity-75          [-1, 384, 14, 14]               0\n",
      "            Block-76          [-1, 384, 14, 14]               0\n",
      "           Conv2d-77          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-78          [-1, 14, 14, 384]             768\n",
      "           Linear-79         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-80         [-1, 14, 14, 1536]               0\n",
      "           Linear-81          [-1, 14, 14, 384]         590,208\n",
      "         Identity-82          [-1, 384, 14, 14]               0\n",
      "            Block-83          [-1, 384, 14, 14]               0\n",
      "           Conv2d-84          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-85          [-1, 14, 14, 384]             768\n",
      "           Linear-86         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-87         [-1, 14, 14, 1536]               0\n",
      "           Linear-88          [-1, 14, 14, 384]         590,208\n",
      "         Identity-89          [-1, 384, 14, 14]               0\n",
      "            Block-90          [-1, 384, 14, 14]               0\n",
      "           Conv2d-91          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-92          [-1, 14, 14, 384]             768\n",
      "           Linear-93         [-1, 14, 14, 1536]         591,360\n",
      "             GELU-94         [-1, 14, 14, 1536]               0\n",
      "           Linear-95          [-1, 14, 14, 384]         590,208\n",
      "         Identity-96          [-1, 384, 14, 14]               0\n",
      "            Block-97          [-1, 384, 14, 14]               0\n",
      "           Conv2d-98          [-1, 384, 14, 14]          19,200\n",
      "        LayerNorm-99          [-1, 14, 14, 384]             768\n",
      "          Linear-100         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-101         [-1, 14, 14, 1536]               0\n",
      "          Linear-102          [-1, 14, 14, 384]         590,208\n",
      "        Identity-103          [-1, 384, 14, 14]               0\n",
      "           Block-104          [-1, 384, 14, 14]               0\n",
      "          Conv2d-105          [-1, 384, 14, 14]          19,200\n",
      "       LayerNorm-106          [-1, 14, 14, 384]             768\n",
      "          Linear-107         [-1, 14, 14, 1536]         591,360\n",
      "            GELU-108         [-1, 14, 14, 1536]               0\n",
      "          Linear-109          [-1, 14, 14, 384]         590,208\n",
      "        Identity-110          [-1, 384, 14, 14]               0\n",
      "           Block-111          [-1, 384, 14, 14]               0\n",
      "       LayerNorm-112          [-1, 384, 14, 14]             768\n",
      "          Conv2d-113            [-1, 768, 7, 7]       1,180,416\n",
      "          Conv2d-114            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-115            [-1, 7, 7, 768]           1,536\n",
      "          Linear-116           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-117           [-1, 7, 7, 3072]               0\n",
      "          Linear-118            [-1, 7, 7, 768]       2,360,064\n",
      "        Identity-119            [-1, 768, 7, 7]               0\n",
      "           Block-120            [-1, 768, 7, 7]               0\n",
      "          Conv2d-121            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-122            [-1, 7, 7, 768]           1,536\n",
      "          Linear-123           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-124           [-1, 7, 7, 3072]               0\n",
      "          Linear-125            [-1, 7, 7, 768]       2,360,064\n",
      "        Identity-126            [-1, 768, 7, 7]               0\n",
      "           Block-127            [-1, 768, 7, 7]               0\n",
      "          Conv2d-128            [-1, 768, 7, 7]          38,400\n",
      "       LayerNorm-129            [-1, 7, 7, 768]           1,536\n",
      "          Linear-130           [-1, 7, 7, 3072]       2,362,368\n",
      "            GELU-131           [-1, 7, 7, 3072]               0\n",
      "          Linear-132            [-1, 7, 7, 768]       2,360,064\n",
      "        Identity-133            [-1, 768, 7, 7]               0\n",
      "           Block-134            [-1, 768, 7, 7]               0\n",
      "       LayerNorm-135                  [-1, 768]           1,536\n",
      "          Linear-136                 [-1, 1000]         769,000\n",
      "================================================================\n",
      "Total params: 28,582,504\n",
      "Trainable params: 28,582,504\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 223.38\n",
      "Params size (MB): 109.03\n",
      "Estimated Total Size (MB): 332.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = convnext_tiny(pretrained=False, in_22k=False, num_classes=1000)\n",
    "summary(model, (3, 224, 224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsupported operator aten::mean encountered 9 time(s)\n",
      "Unsupported operator aten::sub encountered 8 time(s)\n",
      "Unsupported operator aten::pow encountered 4 time(s)\n",
      "Unsupported operator aten::add encountered 26 time(s)\n",
      "Unsupported operator aten::sqrt encountered 4 time(s)\n",
      "Unsupported operator aten::div encountered 4 time(s)\n",
      "Unsupported operator aten::mul encountered 22 time(s)\n",
      "Unsupported operator aten::gelu encountered 18 time(s)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47e+09\n",
      "conv 2.93e+08\n",
      "layer_norm 1.07e+07\n",
      "linear 4.16e+09\n"
     ]
    }
   ],
   "source": [
    "flops = FlopCountAnalysis(model, torch.rand(1, 3, 224, 224))\n",
    "\n",
    "# Format flops.total() in scientific notation\n",
    "formatted_flops_total = \"{:.2e}\".format(flops.total())\n",
    "print(formatted_flops_total)\n",
    "\n",
    "for k,v in flops.by_operator().items():\n",
    "    print(k, \"{:.2e}\".format(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
