{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vit model \n",
    "from typing import Callable, NamedTuple, List, Optional, Union\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import torch\n",
    "import math\n",
    "from torch import nn\n",
    "from torchvision.ops.misc import Conv2dNormActivation, MLP\n",
    "from torchvision.models.vision_transformer import MLPBlock, ConvStemConfig, Encoder\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer as per https://arxiv.org/abs/2010.11929.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size: int,\n",
    "        patch_size: int,\n",
    "        num_layers: int,\n",
    "        num_heads: int,\n",
    "        hidden_dim: int,\n",
    "        n_learned_tokens: int,\n",
    "        mlp_dim: int,\n",
    "        dropout: float = 0.0,\n",
    "        attention_dropout: float = 0.0,\n",
    "        num_classes: int = 1000,\n",
    "        representation_size: Optional[int] = None,\n",
    "        norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "        conv_stem_configs: Optional[List[ConvStemConfig]] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        torch._assert(image_size % patch_size == 0, \"Input shape indivisible by patch size!\")\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_learned_tokens = n_learned_tokens\n",
    "        self.mlp_dim = mlp_dim\n",
    "        self.attention_dropout = attention_dropout\n",
    "        self.dropout = dropout\n",
    "        self.num_classes = num_classes\n",
    "        self.representation_size = representation_size\n",
    "        self.norm_layer = norm_layer\n",
    "\n",
    "        if conv_stem_configs is not None:\n",
    "            # As per https://arxiv.org/abs/2106.14881\n",
    "            seq_proj = nn.Sequential()\n",
    "            prev_channels = 3\n",
    "            for i, conv_stem_layer_config in enumerate(conv_stem_configs):\n",
    "                seq_proj.add_module(\n",
    "                    f\"conv_bn_relu_{i}\",\n",
    "                    Conv2dNormActivation(\n",
    "                        in_channels=prev_channels,\n",
    "                        out_channels=conv_stem_layer_config.out_channels,\n",
    "                        kernel_size=conv_stem_layer_config.kernel_size,\n",
    "                        stride=conv_stem_layer_config.stride,\n",
    "                        norm_layer=conv_stem_layer_config.norm_layer,\n",
    "                        activation_layer=conv_stem_layer_config.activation_layer,\n",
    "                    ),\n",
    "                )\n",
    "                prev_channels = conv_stem_layer_config.out_channels\n",
    "            seq_proj.add_module(\n",
    "                \"conv_last\", nn.Conv2d(in_channels=prev_channels, out_channels=hidden_dim, kernel_size=1)\n",
    "            )\n",
    "            self.conv_proj: nn.Module = seq_proj\n",
    "        else:\n",
    "            self.conv_proj = nn.Conv2d(\n",
    "                in_channels=3, out_channels=hidden_dim, kernel_size=patch_size, stride=patch_size\n",
    "            )\n",
    "\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Add a class token\n",
    "        self.class_token = nn.Parameter(torch.zeros(1, n_learned_tokens, hidden_dim))\n",
    "        seq_length += n_learned_tokens\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            seq_length,\n",
    "            num_layers,\n",
    "            num_heads,\n",
    "            hidden_dim,\n",
    "            mlp_dim,\n",
    "            dropout,\n",
    "            attention_dropout,\n",
    "            norm_layer,\n",
    "        )\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        heads_layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "        if representation_size is None:\n",
    "            heads_layers[\"head\"] = nn.Linear(hidden_dim, num_classes)\n",
    "        else:\n",
    "            heads_layers[\"pre_logits\"] = nn.Linear(hidden_dim, representation_size)\n",
    "            heads_layers[\"act\"] = nn.Tanh()\n",
    "            heads_layers[\"head\"] = nn.Linear(representation_size, num_classes)\n",
    "\n",
    "        self.heads = nn.Sequential(heads_layers)\n",
    "\n",
    "        if isinstance(self.conv_proj, nn.Conv2d):\n",
    "            # Init the patchify stem\n",
    "            fan_in = self.conv_proj.in_channels * self.conv_proj.kernel_size[0] * self.conv_proj.kernel_size[1]\n",
    "            nn.init.trunc_normal_(self.conv_proj.weight, std=math.sqrt(1 / fan_in))\n",
    "            if self.conv_proj.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.bias)\n",
    "        elif self.conv_proj.conv_last is not None and isinstance(self.conv_proj.conv_last, nn.Conv2d):\n",
    "            # Init the last 1x1 conv of the conv stem\n",
    "            nn.init.normal_(\n",
    "                self.conv_proj.conv_last.weight, mean=0.0, std=math.sqrt(2.0 / self.conv_proj.conv_last.out_channels)\n",
    "            )\n",
    "            if self.conv_proj.conv_last.bias is not None:\n",
    "                nn.init.zeros_(self.conv_proj.conv_last.bias)\n",
    "\n",
    "        if hasattr(self.heads, \"pre_logits\") and isinstance(self.heads.pre_logits, nn.Linear):\n",
    "            fan_in = self.heads.pre_logits.in_features\n",
    "            nn.init.trunc_normal_(self.heads.pre_logits.weight, std=math.sqrt(1 / fan_in))\n",
    "            nn.init.zeros_(self.heads.pre_logits.bias)\n",
    "\n",
    "        if isinstance(self.heads.head, nn.Linear):\n",
    "            nn.init.zeros_(self.heads.head.weight)\n",
    "            nn.init.zeros_(self.heads.head.bias)\n",
    "\n",
    "    def _process_input(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        n, c, h, w = x.shape\n",
    "        p = self.patch_size\n",
    "        torch._assert(h == self.image_size, f\"Wrong image height! Expected {self.image_size} but got {h}!\")\n",
    "        torch._assert(w == self.image_size, f\"Wrong image width! Expected {self.image_size} but got {w}!\")\n",
    "        n_h = h // p\n",
    "        n_w = w // p\n",
    "\n",
    "        # (n, c, h, w) -> (n, hidden_dim, n_h, n_w)\n",
    "        x = self.conv_proj(x)\n",
    "        # (n, hidden_dim, n_h, n_w) -> (n, hidden_dim, (n_h * n_w))\n",
    "        x = x.reshape(n, self.hidden_dim, n_h * n_w)\n",
    "\n",
    "        # (n, hidden_dim, (n_h * n_w)) -> (n, (n_h * n_w), hidden_dim)\n",
    "        # The self attention layer expects inputs in the format (N, S, E)\n",
    "        # where S is the source sequence length, N is the batch size, E is the\n",
    "        # embedding dimension\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # Reshape and permute the input tensor\n",
    "        x = self._process_input(x)\n",
    "        n = x.shape[0]\n",
    "\n",
    "        # Expand the class token to the full batch\n",
    "        batch_class_token = self.class_token.expand(n, -1, -1)\n",
    "        x = torch.cat([batch_class_token, x], dim=1)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classifier \"token\" as used by standard language architectures\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "\n",
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertEmbedding, self).__init__()\n",
    "                \n",
    "        bert_id = 'NeuML/pubmedbert-base-embeddings'\n",
    "        self.bert = AutoModel.from_pretrained(bert_id)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(bert_id)\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_ids = self.tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=30)['input_ids']\n",
    "        return self.bert(input_ids).pooler_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = VisionTransformer(image_size=512,\n",
    "        patch_size=16,\n",
    "        num_layers=16,\n",
    "        num_heads=8,\n",
    "        hidden_dim=768,\n",
    "        n_learned_tokens=32,\n",
    "        mlp_dim=1280)\n",
    "\n",
    "bert_model = BertEmbedding()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "class CrossAttentionModel(nn.Module):\n",
    "    def __init__(self, d_h=768, d_g=768, d=768, K=32, M=8, temperature=0.07, use_p_tokens=False):\n",
    "        \"\"\"\n",
    "        d_h: Dimension of each visual mixture token embedding (determined by ViT).\n",
    "        d_g: Dimension of text encoder embeddings.\n",
    "        d: Output dimension after cross-attention.\n",
    "        K: Number of learnable tokens for cross-attention.\n",
    "        M: Number of attention heads.\n",
    "        temperature: Temperature for the softmax operation.\n",
    "        \"\"\"\n",
    "        super(CrossAttentionModel, self).__init__()\n",
    "        \n",
    "        # Parameters\n",
    "        self.K = K\n",
    "        self.M = M\n",
    "        self.d = d\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Initialize ViT as the image encoder\n",
    "        self.image_encoder = VisionTransformer(image_size=512,\n",
    "                                                    patch_size=16,\n",
    "                                                    num_layers=16,\n",
    "                                                    num_heads=M,\n",
    "                                                    hidden_dim=d_h,\n",
    "                                                    n_learned_tokens=K,\n",
    "                                                    mlp_dim=d_h)\n",
    "        self.d_h = d_h\n",
    "        self.d_g = d_g\n",
    "        self.use_p_tokens = use_p_tokens\n",
    "        # Linear transformations for multi-head cross-attention\n",
    "        self.W_q = nn.Parameter(torch.randn(self.d_g, self.d))      # Query projection for each head\n",
    "        self.W_k = nn.Parameter(torch.randn(self.d_h, self.d))      # Key projection for each head\n",
    "        self.W_v = nn.Parameter(torch.randn(self.d_h, self.d))      # Value projection for each head\n",
    "        self.W_o = nn.Parameter(torch.randn(self.d, self.d))        # Final output projection\n",
    "\n",
    "        # Placeholder for text encoder (can be any pretrained text model)\n",
    "        self.text_encoder = BertEmbedding()             # Replace with actual text encoder\n",
    "        \n",
    "        # Projection matrix for text vector\n",
    "        self.W_T = nn.Parameter(torch.randn(d_g, d))\n",
    "        self.a = torch.log(torch.tensor(10.0))\n",
    "        self.b = torch.tensor(-10.0)\n",
    "\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        \"\"\"\n",
    "        image: Tensor representing an image, with shape (batch_size, channels, height, width).\n",
    "        text: Tensor representing a text caption, with shape (batch_size, d_g).\n",
    "        \"\"\"\n",
    "        batch_size = image.size(0)\n",
    "\n",
    "\n",
    "        # Step 2: Pass the image and the learnable tokens through the ViT\n",
    "        # Concatenate the learnable tokens to the patch embeddings as extra tokens\n",
    "        h_i = self.image_encoder(image)          # (batch_size, N, d_h) N = Numer of patches + K\n",
    "        if not self.use_p_tokens:\n",
    "            # Extract only the learnable token outputs as visual mixture tokens\n",
    "            h_i = h_i[:, :self.K, :]                               # (batch_size, K, d_h)\n",
    "\n",
    "        # Step 3: Compute text representation\n",
    "        g_j = self.text_encoder(text)                          # (batch_size, d_g)\n",
    "\n",
    "        # Step 4: Project text representation for each head to obtain queries\n",
    "        Q_j = torch.einsum('bg,gd->bd', g_j, self.W_q)        # (batch_size, d)\n",
    "\n",
    "        # Step 5: Project each visual mixture token for keys and values\n",
    "        K_i = torch.einsum('bkd,dm->bkm', h_i, self.W_k)       # (batch_size, K, d)\n",
    "        V_i = torch.einsum('bkd,dm->bkm', h_i, self.W_v)       # (batch_size, K, d)\n",
    "        \n",
    "\n",
    "        Q_j = Q_j.view(batch_size, self.M, self.d // self.M)  # (batch_size, M, d // M)\n",
    "        K_i = K_i.view(batch_size, self.K, self.M, self.d // self.M)  # (batch_size, K, M, d // M)\n",
    "        V_i = V_i.view(batch_size, self.K, self.M, self.d // self.M)  # (batch_size, K, M, d // M)\n",
    "        print(\"Multi-head queries (text)\", Q_j.shape, \"Multi-head Keys(image)\", K_i.shape, \"multi-head values(images)\",  V_i.shape, \"expected (batch_size, M, d // M), (batch_size, K, M, d // M), (batch_size, K, M, d // M)\")\n",
    "        # Step 6: Calculate attention scores for each head and mixture token\n",
    "        attention_logits = torch.einsum('bmd,ckmd->bckm', Q_j, K_i)  # (batch_size, batch_size, K, M)\n",
    "        print(\"attention_logits\", attention_logits.shape, \"expected (batch_size, batch_size, K, M)\")\n",
    "        attention_weights = F.softmax(attention_logits / self.temperature, dim=1)  # (batch_size, batch_size, K, M)\n",
    "\n",
    "        # Step 7: Apply attention weights to values\n",
    "        weighted_values = torch.einsum('bckm,bkmd->bkmd', attention_weights, V_i).sum(dim=1)  # (batch_size, K, M, d // M)\n",
    "        # weighted_values = (attention_weights * V_i).sum(dim=1)  # (batch_size, M, d // M)\n",
    "        print(\"weighted value\", weighted_values.shape, \"expected (batch_size, M, d // M)\")\n",
    "        # Step 8: Concatenate heads and project to final dimension d\n",
    "        z_ij = weighted_values.view(batch_size, -1)           # (batch_size, d)\n",
    "        z_ij = torch.matmul(z_ij, self.W_o)                   # (batch_size, d)\n",
    "\n",
    "        # Step 9: Project text vector\n",
    "        z_j = torch.matmul(g_j, self.W_T)                     # (batch_size, d)\n",
    "\n",
    "        # Step 10: Normalize both representations\n",
    "        z_ij = F.normalize(z_ij, p=2, dim=1)                  # (batch_size, d)\n",
    "        z_j = F.normalize(z_j, p=2, dim=1)                    # (batch_size, d)\n",
    "\n",
    "        return z_ij, z_j\n",
    "    \n",
    "    def llip_loss(self, Z, Z_prime):\n",
    "        \"\"\"\n",
    "        Compute the Llip loss from text-conditioned image embeddings and text embeddings.\n",
    "\n",
    "        Args:\n",
    "            Z (torch.Tensor): Tensor of shape (batch_size, d) representing embeddings for the first modality (e.g., images).\n",
    "            Z_prime (torch.Tensor): Tensor of shape (batch_size, d) representing embeddings for the second modality (e.g., text).\n",
    "            a (float): Scaling factor for the similarities.\n",
    "            b (float): Shifting factor for the similarities.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed L_Llip loss.\n",
    "        \"\"\"\n",
    "        batch_size = Z.size(0)\n",
    "        a = torch.exp(self.a)\n",
    "        logits = torch.einsum('ab,cd->ac', Z, Z_prime)  # (batch_size, batch_size)\n",
    "        logits = (a * logits) + self.b\n",
    "        pos_ids = torch.eye(batch_size)\n",
    "        neg_ids = 1 - pos_ids\n",
    "        pos_logits = logits * pos_ids\n",
    "        \n",
    "        neg_logits = logits * neg_ids\n",
    "        pos_loss = -F.logsigmoid(pos_logits)\n",
    "        neg_loss = -F.logsigmoid(-neg_logits).mean(dim=1)\n",
    "        llip_loss_value = (pos_loss.sum() + neg_loss.sum()) / batch_size\n",
    "\n",
    "\n",
    "        return llip_loss_value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-head queries (text) torch.Size([4, 8, 96]) Multi-head Keys(image) torch.Size([4, 32, 8, 96]) multi-head values(images) torch.Size([4, 32, 8, 96]) expected (batch_size, M, d // M), (batch_size, K, M, d // M), (batch_size, K, M, d // M)\n",
      "attention_logits torch.Size([4, 4, 32, 8]) expected (batch_size, batch_size, K, M)\n",
      "weighted value torch.Size([4, 8, 96]) expected (batch_size, M, d // M)\n"
     ]
    }
   ],
   "source": [
    "text = [\"aboba\", \"aboba\", \"aboba\", \"aboba\"]\n",
    "image = torch.rand(4, 3, 512, 512)\n",
    "model = CrossAttentionModel()\n",
    "z_ij, z_j = model(image, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llip_loss(Z, Z_prime, a=1.0, b=0.0):\n",
    "        \"\"\"\n",
    "        Compute the Llip loss from text-conditioned image embeddings and text embeddings.\n",
    "\n",
    "        Args:\n",
    "            Z (torch.Tensor): Tensor of shape (batch_size, d) representing embeddings for the first modality (e.g., images).\n",
    "            Z_prime (torch.Tensor): Tensor of shape (batch_size, d) representing embeddings for the second modality (e.g., text).\n",
    "            a (float): Scaling factor for the similarities.\n",
    "            b (float): Shifting factor for the similarities.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The computed L_Llip loss.\n",
    "        \"\"\"\n",
    "        batch_size = Z.size(0)\n",
    "\n",
    "        logits = torch.einsum('ab,cd->ac', Z, Z_prime)  # (batch_size, batch_size)\n",
    "        logits = (a * logits) + b\n",
    "        pos_ids = torch.eye(batch_size)\n",
    "        neg_ids = 1 - pos_ids\n",
    "        pos_logits = logits * pos_ids\n",
    "        print(pos_logits)\n",
    "        \n",
    "        neg_logits = logits * neg_ids\n",
    "        print(neg_logits)\n",
    "        pos_loss = -F.logsigmoid(pos_logits)\n",
    "        neg_loss = -F.logsigmoid(-neg_logits).mean(dim=1)\n",
    "        llip_loss_value = (pos_loss.sum() + neg_loss.sum()) / batch_size\n",
    "\n",
    "\n",
    "        return llip_loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
